{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Corp.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Qsv_mOTAw_Ou",
        "eoPn0pWMw_O0",
        "VsyCAwdYw_O8",
        "YVx2vHHew_PB"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQeOVzhfw_OS",
        "outputId": "9925045f-9124-4e85-958b-ff8b467ed661"
      },
      "source": [
        "from sklearn.datasets import load_files       \n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "# define function to load train, test, and validation datasets\n",
        "def load_dataset(path):\n",
        "    data = load_files(path)\n",
        "    dog_files = np.array(data['filenames'])\n",
        "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
        "    return dog_files, dog_targets\n",
        "\n",
        "# load train, test, and validation datasets\n",
        "train_files, train_targets = load_dataset('/data/dog_images/train')\n",
        "valid_files, valid_targets = load_dataset('/data/dog_images/valid')\n",
        "test_files, test_targets = load_dataset('/data/dog_images/test')\n",
        "\n",
        "# load list of dog names\n",
        "dog_names = [item[20:-1] for item in sorted(glob(\"/data/dog_images/train/*/\"))]\n",
        "\n",
        "# print statistics about the dataset\n",
        "print('There are %d total dog categories.' % len(dog_names))\n",
        "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
        "print('There are %d training dog images.' % len(train_files))\n",
        "print('There are %d validation dog images.' % len(valid_files))\n",
        "print('There are %d test dog images.'% len(test_files))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "There are 133 total dog categories.\n",
            "There are 8351 total dog images.\n",
            "\n",
            "There are 6680 training dog images.\n",
            "There are 835 validation dog images.\n",
            "There are 836 test dog images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CoqgDjLw_Oc",
        "outputId": "6dba3335-b66e-4e10-d81e-cb7eef42c4ee"
      },
      "source": [
        "human_files_short = human_files[:100]\n",
        "dog_files_short = train_files[:100]\n",
        "# Do NOT modify the code above this line.\n",
        "\n",
        "## TODO: Test the performance of the face_detector algorithm \n",
        "## on the images in human_files_short and dog_files_short.\n",
        "count_humans = 0 \n",
        "count_dogs = 0\n",
        "\n",
        "for file in human_files_short:\n",
        "    if face_detector(file) == True:\n",
        "        count_humans += 1\n",
        "        \n",
        "for file in dog_files_short:\n",
        "    if face_detector(file) == True:\n",
        "        count_dogs += 1\n",
        "\n",
        "print('%.1f%% images of the first 100 human_files were detected as human face.' % count_humans)\n",
        "print('%.1f%% images of the first 100 dog_files were detected as human face.' % count_dogs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.0% images of the first 100 human_files were detected as human face.\n",
            "11.0% images of the first 100 dog_files were detected as human face.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9AOe4CrDw_Of"
      },
      "source": [
        "## (Optional) TODO: Report the performance of another  \n",
        "## face detection algorithm on the LFW dataset\n",
        "### Feel free to use as many code cells as needed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDt8KbREw_Oh",
        "outputId": "1c7f73b7-d9d4-4d3a-ce0a-44d252b89cf0"
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "# define ResNet50 model\n",
        "ResNet50_model = ResNet50(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102858752/102853048 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZziXRNvgw_Ok"
      },
      "source": [
        "from keras.preprocessing import image                  \n",
        "from tqdm import tqdm\n",
        "\n",
        "def path_to_tensor(img_path):\n",
        "    # loads RGB image as PIL.Image.Image type\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
        "    x = image.img_to_array(img)\n",
        "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "def paths_to_tensor(img_paths):\n",
        "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
        "    return np.vstack(list_of_tensors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVUuYRE4w_Ot",
        "outputId": "12aec149-3e80-4243-c1db-749ddbd03502"
      },
      "source": [
        "from PIL import ImageFile                            \n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
        "\n",
        "# pre-process the data for Keras\n",
        "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
        "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
        "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6680/6680 [01:27<00:00, 48.97it/s] \n",
            "100%|██████████| 835/835 [00:09<00:00, 84.19it/s] \n",
            "100%|██████████| 836/836 [00:09<00:00, 84.23it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsv_mOTAw_Ou"
      },
      "source": [
        "\n",
        "    \n",
        "        model.summary()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY_KKl7iw_Ou",
        "outputId": "d792cad2-25a4-414d-8fd4-b865daab8a05"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "### TODO: Define your architecture.\n",
        "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(133, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 224, 224, 16)      208       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 112, 112, 32)      2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 56, 56, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               25690624  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 133)               68229     \n",
            "=================================================================\n",
            "Total params: 25,769,397\n",
            "Trainable params: 25,769,397\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2xF0tvxw_Ov"
      },
      "source": [
        "### Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jstB_yjw_Ov"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtOZtkjnw_Ow",
        "outputId": "afbcf855-4bee-4c75-d373-e6148a85e06b"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint  \n",
        "\n",
        "### TODO: specify the number of epochs that you would like to use to train the model.\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "### Do NOT modify the code below this line.\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit(train_tensors, train_targets, \n",
        "          validation_data=(valid_tensors, valid_targets),\n",
        "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6680 samples, validate on 835 samples\n",
            "Epoch 1/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 4.8892 - acc: 0.0141Epoch 00001: val_loss improved from inf to 4.63711, saving model to saved_models/weights.best.from_scratch.hdf5\n",
            "6680/6680 [==============================] - 32s 5ms/step - loss: 4.8888 - acc: 0.0142 - val_loss: 4.6371 - val_acc: 0.0407\n",
            "Epoch 2/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 4.4284 - acc: 0.0535Epoch 00002: val_loss improved from 4.63711 to 4.31164, saving model to saved_models/weights.best.from_scratch.hdf5\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 4.4280 - acc: 0.0534 - val_loss: 4.3116 - val_acc: 0.0623\n",
            "Epoch 3/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 3.9312 - acc: 0.1200Epoch 00003: val_loss improved from 4.31164 to 4.18914, saving model to saved_models/weights.best.from_scratch.hdf5\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 3.9315 - acc: 0.1198 - val_loss: 4.1891 - val_acc: 0.0802\n",
            "Epoch 4/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 3.2054 - acc: 0.2506Epoch 00004: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 3.2068 - acc: 0.2506 - val_loss: 4.2691 - val_acc: 0.0850\n",
            "Epoch 5/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 2.3555 - acc: 0.4215Epoch 00005: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 2.3539 - acc: 0.4219 - val_loss: 4.7338 - val_acc: 0.0826\n",
            "Epoch 6/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 1.5322 - acc: 0.6086Epoch 00006: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 1.5343 - acc: 0.6082 - val_loss: 5.0967 - val_acc: 0.0862\n",
            "Epoch 7/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.9493 - acc: 0.7512Epoch 00007: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.9492 - acc: 0.7515 - val_loss: 6.2125 - val_acc: 0.0802\n",
            "Epoch 8/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.6458 - acc: 0.8315Epoch 00008: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.6462 - acc: 0.8311 - val_loss: 6.7832 - val_acc: 0.0814\n",
            "Epoch 9/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.8833Epoch 00009: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.4578 - acc: 0.8832 - val_loss: 7.1258 - val_acc: 0.0826\n",
            "Epoch 10/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.9060Epoch 00010: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.3527 - acc: 0.9063 - val_loss: 7.5833 - val_acc: 0.0766\n",
            "Epoch 11/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.9155Epoch 00011: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.3286 - acc: 0.9154 - val_loss: 7.5342 - val_acc: 0.0802\n",
            "Epoch 12/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9336Epoch 00012: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.2696 - acc: 0.9338 - val_loss: 7.6679 - val_acc: 0.0814\n",
            "Epoch 13/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9347Epoch 00013: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.2442 - acc: 0.9347 - val_loss: 8.4367 - val_acc: 0.0838\n",
            "Epoch 14/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9417Epoch 00014: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.2381 - acc: 0.9416 - val_loss: 8.9765 - val_acc: 0.0826\n",
            "Epoch 15/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9450Epoch 00015: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.2052 - acc: 0.9449 - val_loss: 8.0810 - val_acc: 0.0802\n",
            "Epoch 16/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9521Epoch 00016: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.2013 - acc: 0.9519 - val_loss: 8.3775 - val_acc: 0.0802\n",
            "Epoch 17/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9514Epoch 00017: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.2084 - acc: 0.9510 - val_loss: 7.4675 - val_acc: 0.0826\n",
            "Epoch 18/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9569Epoch 00018: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.1872 - acc: 0.9569 - val_loss: 8.4210 - val_acc: 0.0838\n",
            "Epoch 19/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9532Epoch 00019: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.1960 - acc: 0.9531 - val_loss: 9.5485 - val_acc: 0.0958\n",
            "Epoch 20/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9514Epoch 00020: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.2075 - acc: 0.9515 - val_loss: 8.8581 - val_acc: 0.0958\n",
            "Epoch 21/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9584Epoch 00021: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.1857 - acc: 0.9585 - val_loss: 8.9605 - val_acc: 0.0874\n",
            "Epoch 22/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9544Epoch 00022: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.2032 - acc: 0.9542 - val_loss: 8.5786 - val_acc: 0.0814\n",
            "Epoch 23/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9593Epoch 00023: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.1821 - acc: 0.9594 - val_loss: 9.3286 - val_acc: 0.0778\n",
            "Epoch 24/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9584Epoch 00024: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.1927 - acc: 0.9581 - val_loss: 10.9326 - val_acc: 0.0802\n",
            "Epoch 25/25\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9535Epoch 00025: val_loss did not improve\n",
            "6680/6680 [==============================] - 31s 5ms/step - loss: 0.2037 - acc: 0.9534 - val_loss: 9.7961 - val_acc: 0.0862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f45b939a8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN8zq8esw_Ox"
      },
      "source": [
        "### Load the Model with the Best Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmDubylCw_Ox"
      },
      "source": [
        "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd_LOmQEw_Oy",
        "outputId": "0a9a22c7-c8f9-450c-c6c2-88653cbe7087"
      },
      "source": [
        "# get index of predicted dog breed for each image in test set\n",
        "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
        "\n",
        "# report test accuracy\n",
        "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
        "print('Test accuracy: %.4f%%' % test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 6.5789%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOsMeLYCw_Oz"
      },
      "source": [
        "bottleneck_features = np.load('/data/bottleneck_features/DogVGG16Data.npz')\n",
        "train_VGG16 = bottleneck_features['train']\n",
        "valid_VGG16 = bottleneck_features['valid']\n",
        "test_VGG16 = bottleneck_features['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoPn0pWMw_O0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfMXpSnjw_O0",
        "outputId": "30870917-5a6a-4f6e-8a63-99fdae0b871b"
      },
      "source": [
        "VGG16_model = Sequential()\n",
        "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
        "VGG16_model.add(Dense(133, activation='softmax'))\n",
        "\n",
        "VGG16_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 133)               68229     \n",
            "=================================================================\n",
            "Total params: 68,229\n",
            "Trainable params: 68,229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYcj0b2Xw_O1"
      },
      "source": [
        "### Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U0G1sTTw_O1"
      },
      "source": [
        "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHyiw16sw_O2"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrk8B3yrw_O3",
        "outputId": "30c88d67-afdf-42d3-af43-61b17cfdbdda"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "VGG16_model.fit(train_VGG16, train_targets, \n",
        "          validation_data=(valid_VGG16, valid_targets),\n",
        "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6680 samples, validate on 835 samples\n",
            "Epoch 1/20\n",
            "6480/6680 [============================>.] - ETA: 0s - loss: 11.8245 - acc: 0.1397Epoch 00001: val_loss improved from inf to 10.08498, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 329us/step - loss: 11.7808 - acc: 0.1410 - val_loss: 10.0850 - val_acc: 0.2263\n",
            "Epoch 2/20\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 9.2921 - acc: 0.3205Epoch 00002: val_loss improved from 10.08498 to 9.23901, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 265us/step - loss: 9.3018 - acc: 0.3202 - val_loss: 9.2390 - val_acc: 0.3162\n",
            "Epoch 3/20\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 8.6830 - acc: 0.3883Epoch 00003: val_loss improved from 9.23901 to 8.74831, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 265us/step - loss: 8.6822 - acc: 0.3877 - val_loss: 8.7483 - val_acc: 0.3509\n",
            "Epoch 4/20\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 8.2057 - acc: 0.4370Epoch 00004: val_loss improved from 8.74831 to 8.63532, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 264us/step - loss: 8.2155 - acc: 0.4364 - val_loss: 8.6353 - val_acc: 0.3832\n",
            "Epoch 5/20\n",
            "6520/6680 [============================>.] - ETA: 0s - loss: 8.0668 - acc: 0.4598Epoch 00005: val_loss improved from 8.63532 to 8.48461, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 267us/step - loss: 8.0749 - acc: 0.4585 - val_loss: 8.4846 - val_acc: 0.3952\n",
            "Epoch 6/20\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 7.9400 - acc: 0.4764Epoch 00006: val_loss improved from 8.48461 to 8.40785, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 264us/step - loss: 7.9426 - acc: 0.4763 - val_loss: 8.4078 - val_acc: 0.4000\n",
            "Epoch 7/20\n",
            "6540/6680 [============================>.] - ETA: 0s - loss: 7.8360 - acc: 0.4927Epoch 00007: val_loss did not improve\n",
            "6680/6680 [==============================] - 2s 265us/step - loss: 7.8536 - acc: 0.4907 - val_loss: 8.4177 - val_acc: 0.4084\n",
            "Epoch 8/20\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 7.7954 - acc: 0.4962Epoch 00008: val_loss improved from 8.40785 to 8.29893, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 266us/step - loss: 7.7686 - acc: 0.4979 - val_loss: 8.2989 - val_acc: 0.4024\n",
            "Epoch 9/20\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 7.5962 - acc: 0.5048Epoch 00009: val_loss improved from 8.29893 to 8.10031, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 266us/step - loss: 7.5955 - acc: 0.5048 - val_loss: 8.1003 - val_acc: 0.4060\n",
            "Epoch 10/20\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 7.4502 - acc: 0.5189Epoch 00010: val_loss improved from 8.10031 to 8.07770, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 263us/step - loss: 7.4357 - acc: 0.5201 - val_loss: 8.0777 - val_acc: 0.4311\n",
            "Epoch 11/20\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 7.4054 - acc: 0.5280Epoch 00011: val_loss did not improve\n",
            "6680/6680 [==============================] - 2s 266us/step - loss: 7.3955 - acc: 0.5283 - val_loss: 8.1388 - val_acc: 0.4192\n",
            "Epoch 12/20\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 7.3082 - acc: 0.5337Epoch 00012: val_loss improved from 8.07770 to 7.99200, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 266us/step - loss: 7.3266 - acc: 0.5328 - val_loss: 7.9920 - val_acc: 0.4311\n",
            "Epoch 13/20\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 7.2449 - acc: 0.5405Epoch 00013: val_loss improved from 7.99200 to 7.98137, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 265us/step - loss: 7.2622 - acc: 0.5395 - val_loss: 7.9814 - val_acc: 0.4335\n",
            "Epoch 14/20\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 7.1681 - acc: 0.5441Epoch 00014: val_loss improved from 7.98137 to 7.89974, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 265us/step - loss: 7.1747 - acc: 0.5437 - val_loss: 7.8997 - val_acc: 0.4335\n",
            "Epoch 15/20\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 7.1009 - acc: 0.5521Epoch 00015: val_loss improved from 7.89974 to 7.88508, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 265us/step - loss: 7.1200 - acc: 0.5510 - val_loss: 7.8851 - val_acc: 0.4503\n",
            "Epoch 16/20\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 7.0541 - acc: 0.5505Epoch 00016: val_loss improved from 7.88508 to 7.71263, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 264us/step - loss: 7.0466 - acc: 0.5509 - val_loss: 7.7126 - val_acc: 0.4515\n",
            "Epoch 17/20\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 6.8522 - acc: 0.5590Epoch 00017: val_loss improved from 7.71263 to 7.54916, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 265us/step - loss: 6.8259 - acc: 0.5608 - val_loss: 7.5492 - val_acc: 0.4587\n",
            "Epoch 18/20\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 6.6554 - acc: 0.5708Epoch 00018: val_loss improved from 7.54916 to 7.38351, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 263us/step - loss: 6.6419 - acc: 0.5716 - val_loss: 7.3835 - val_acc: 0.4563\n",
            "Epoch 19/20\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 6.4831 - acc: 0.5845Epoch 00019: val_loss improved from 7.38351 to 7.32043, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 265us/step - loss: 6.4856 - acc: 0.5844 - val_loss: 7.3204 - val_acc: 0.4766\n",
            "Epoch 20/20\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 6.3768 - acc: 0.5918Epoch 00020: val_loss improved from 7.32043 to 7.27574, saving model to saved_models/weights.best.VGG16.hdf5\n",
            "6680/6680 [==============================] - 2s 264us/step - loss: 6.3776 - acc: 0.5919 - val_loss: 7.2757 - val_acc: 0.4754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f45b9670278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JHfqgDiw_O3"
      },
      "source": [
        "### Load the Model with the Best Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s7qWaasw_O4"
      },
      "source": [
        "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCZBKqVLw_O4"
      },
      "source": [
        "### Test the Model\n",
        "\n",
        "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PviaYKqAw_O5",
        "outputId": "8e5c3bb0-6eb7-4bc3-eab8-c8271c8e2387"
      },
      "source": [
        "# get index of predicted dog breed for each image in test set\n",
        "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
        "\n",
        "# report test accuracy\n",
        "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
        "print('Test accuracy: %.4f%%' % test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 47.8469%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV7_oTXBw_O5"
      },
      "source": [
        "### Predict Dog Breed with the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHg9VFYZw_O6"
      },
      "source": [
        "from extract_bottleneck_features import *\n",
        "\n",
        "def VGG16_predict_breed(img_path):\n",
        "    # extract bottleneck features\n",
        "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
        "    # obtain predicted vector\n",
        "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
        "    # return dog breed that is predicted by the model\n",
        "    return dog_names[np.argmax(predicted_vector)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7qtdwXiw_O6"
      },
      "source": [
        "\n",
        "    Dog{network}Data.npz\n",
        "    \n",
        "\n",
        "    bottleneck_features = np.load('/data/bottleneck_features/Dog{network}Data.npz')\n",
        "    train_{network} = bottleneck_features['train']\n",
        "    valid_{network} = bottleneck_features['valid']\n",
        "    test_{network} = bottleneck_features['test']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgR1PU5Cw_O7"
      },
      "source": [
        "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
        "bottleneck_features = np.load('/data/bottleneck_features/DogXceptionData.npz')\n",
        "train_Xception = bottleneck_features['train']\n",
        "valid_Xception = bottleneck_features['valid']\n",
        "test_Xception = bottleneck_features['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsyCAwdYw_O8"
      },
      "source": [
        "\n",
        "    \n",
        "        <your model's name>.summary()\n",
        "   \n",
        "_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJGKjoQ1w_O8",
        "outputId": "1f24fcdc-f658-4cad-a6fe-8711f62429f7"
      },
      "source": [
        "### TODO: Define your architecture.\n",
        "from keras.layers import Dropout\n",
        "\n",
        "Xception_model = Sequential()\n",
        "Xception_model.add(GlobalAveragePooling2D(input_shape=train_Xception.shape[1:]))\n",
        "Xception_model.add(Dense(500, activation='relu'))\n",
        "Xception_model.add(Dropout(0.4))\n",
        "Xception_model.add(Dense(133, activation='softmax'))\n",
        "\n",
        "Xception_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 500)               1024500   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 133)               66633     \n",
            "=================================================================\n",
            "Total params: 1,091,133\n",
            "Trainable params: 1,091,133\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_CSIxNcw_O8"
      },
      "source": [
        "### (IMPLEMENTATION) Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmaL_DbUw_O9"
      },
      "source": [
        "### TODO: Compile the model.\n",
        "Xception_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiU1Tojhw_O-"
      },
      "source": [
        "### (IMPLEMENTATION) Train the Model\n",
        "\n",
        "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
        "\n",
        "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1puCl4Lw_O-",
        "outputId": "53915886-7cf2-4265-98e7-bdc716ab53a1"
      },
      "source": [
        "### TODO: Train the model.\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "Xception_model.fit(train_Xception, train_targets, \n",
        "          validation_data=(valid_Xception, valid_targets),\n",
        "          epochs=100, batch_size=20, callbacks=[checkpointer], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6680 samples, validate on 835 samples\n",
            "Epoch 1/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 1.4276 - acc: 0.6498Epoch 00001: val_loss improved from inf to 0.63011, saving model to saved_models/weights.best.Xception.hdf5\n",
            "6680/6680 [==============================] - 4s 584us/step - loss: 1.4175 - acc: 0.6507 - val_loss: 0.6301 - val_acc: 0.7988\n",
            "Epoch 2/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.6543 - acc: 0.7991Epoch 00002: val_loss improved from 0.63011 to 0.57589, saving model to saved_models/weights.best.Xception.hdf5\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.6552 - acc: 0.7990 - val_loss: 0.5759 - val_acc: 0.8228\n",
            "Epoch 3/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8393Epoch 00003: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.5347 - acc: 0.8395 - val_loss: 0.6260 - val_acc: 0.8240\n",
            "Epoch 4/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.4325 - acc: 0.8664Epoch 00004: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 528us/step - loss: 0.4329 - acc: 0.8663 - val_loss: 0.6192 - val_acc: 0.8323\n",
            "Epoch 5/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8803Epoch 00005: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 520us/step - loss: 0.3951 - acc: 0.8799 - val_loss: 0.6121 - val_acc: 0.8323\n",
            "Epoch 6/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8957Epoch 00006: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 523us/step - loss: 0.3383 - acc: 0.8963 - val_loss: 0.5950 - val_acc: 0.8407\n",
            "Epoch 7/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.3112 - acc: 0.9021Epoch 00007: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 521us/step - loss: 0.3114 - acc: 0.9015 - val_loss: 0.6616 - val_acc: 0.8443\n",
            "Epoch 8/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9182Epoch 00008: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.2771 - acc: 0.9183 - val_loss: 0.7242 - val_acc: 0.8383\n",
            "Epoch 9/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9206Epoch 00009: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 524us/step - loss: 0.2577 - acc: 0.9208 - val_loss: 0.7389 - val_acc: 0.8455\n",
            "Epoch 10/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9294Epoch 00010: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 529us/step - loss: 0.2348 - acc: 0.9293 - val_loss: 0.8303 - val_acc: 0.8251\n",
            "Epoch 11/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9321Epoch 00011: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 530us/step - loss: 0.2141 - acc: 0.9323 - val_loss: 0.7994 - val_acc: 0.8419\n",
            "Epoch 12/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9383Epoch 00012: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 527us/step - loss: 0.2107 - acc: 0.9380 - val_loss: 0.8403 - val_acc: 0.8467\n",
            "Epoch 13/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9433Epoch 00013: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 527us/step - loss: 0.1907 - acc: 0.9430 - val_loss: 0.8932 - val_acc: 0.8371\n",
            "Epoch 14/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9520Epoch 00014: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 518us/step - loss: 0.1700 - acc: 0.9521 - val_loss: 0.8737 - val_acc: 0.8323\n",
            "Epoch 15/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9500Epoch 00015: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 521us/step - loss: 0.1645 - acc: 0.9500 - val_loss: 0.8013 - val_acc: 0.8503\n",
            "Epoch 16/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9517Epoch 00016: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 519us/step - loss: 0.1689 - acc: 0.9515 - val_loss: 0.8971 - val_acc: 0.8515\n",
            "Epoch 17/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9532Epoch 00017: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.1541 - acc: 0.9533 - val_loss: 0.9977 - val_acc: 0.8467\n",
            "Epoch 18/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9574Epoch 00018: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 521us/step - loss: 0.1545 - acc: 0.9573 - val_loss: 0.9477 - val_acc: 0.8467\n",
            "Epoch 19/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9594Epoch 00019: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.1548 - acc: 0.9588 - val_loss: 1.1059 - val_acc: 0.8419\n",
            "Epoch 20/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9624Epoch 00020: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 524us/step - loss: 0.1310 - acc: 0.9623 - val_loss: 1.0591 - val_acc: 0.8455\n",
            "Epoch 21/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9657Epoch 00021: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 523us/step - loss: 0.1263 - acc: 0.9657 - val_loss: 1.1031 - val_acc: 0.8395\n",
            "Epoch 22/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9629Epoch 00022: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 514us/step - loss: 0.1310 - acc: 0.9630 - val_loss: 1.1020 - val_acc: 0.8323\n",
            "Epoch 23/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9668Epoch 00023: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.1057 - acc: 0.9669 - val_loss: 1.0368 - val_acc: 0.8503\n",
            "Epoch 24/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9664Epoch 00024: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.1253 - acc: 0.9666 - val_loss: 1.0942 - val_acc: 0.8431\n",
            "Epoch 25/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9654Epoch 00025: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 519us/step - loss: 0.1262 - acc: 0.9656 - val_loss: 1.0748 - val_acc: 0.8455\n",
            "Epoch 26/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9713Epoch 00026: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.1075 - acc: 0.9716 - val_loss: 1.1157 - val_acc: 0.8371\n",
            "Epoch 27/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9713Epoch 00027: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.1154 - acc: 0.9714 - val_loss: 1.1710 - val_acc: 0.8347\n",
            "Epoch 28/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9691Epoch 00028: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 521us/step - loss: 0.1152 - acc: 0.9690 - val_loss: 1.1805 - val_acc: 0.8395\n",
            "Epoch 29/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9752Epoch 00029: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.0966 - acc: 0.9753 - val_loss: 1.2063 - val_acc: 0.8371\n",
            "Epoch 30/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9745Epoch 00030: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 520us/step - loss: 0.1049 - acc: 0.9746 - val_loss: 1.2603 - val_acc: 0.8347\n",
            "Epoch 31/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9758Epoch 00031: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.1005 - acc: 0.9756 - val_loss: 1.1746 - val_acc: 0.8443\n",
            "Epoch 32/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9736Epoch 00032: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.1017 - acc: 0.9735 - val_loss: 1.2060 - val_acc: 0.8491\n",
            "Epoch 33/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9755Epoch 00033: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 517us/step - loss: 0.0924 - acc: 0.9754 - val_loss: 1.2566 - val_acc: 0.8407\n",
            "Epoch 34/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9757Epoch 00034: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 527us/step - loss: 0.0838 - acc: 0.9759 - val_loss: 1.2942 - val_acc: 0.8467\n",
            "Epoch 35/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9742Epoch 00035: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.1036 - acc: 0.9741 - val_loss: 1.2888 - val_acc: 0.8335\n",
            "Epoch 36/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9755Epoch 00036: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.0910 - acc: 0.9754 - val_loss: 1.3336 - val_acc: 0.8395\n",
            "Epoch 37/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9805Epoch 00037: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 520us/step - loss: 0.0764 - acc: 0.9805 - val_loss: 1.4054 - val_acc: 0.8383\n",
            "Epoch 38/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9777Epoch 00038: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 524us/step - loss: 0.0932 - acc: 0.9775 - val_loss: 1.2815 - val_acc: 0.8479\n",
            "Epoch 39/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9770Epoch 00039: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0900 - acc: 0.9771 - val_loss: 1.3458 - val_acc: 0.8347\n",
            "Epoch 40/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9809Epoch 00040: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 523us/step - loss: 0.0717 - acc: 0.9808 - val_loss: 1.3212 - val_acc: 0.8587\n",
            "Epoch 41/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9792Epoch 00041: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 523us/step - loss: 0.0824 - acc: 0.9793 - val_loss: 1.3379 - val_acc: 0.8467\n",
            "Epoch 42/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9800Epoch 00042: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0820 - acc: 0.9799 - val_loss: 1.4798 - val_acc: 0.8359\n",
            "Epoch 43/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9818Epoch 00043: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0786 - acc: 0.9817 - val_loss: 1.3491 - val_acc: 0.8359\n",
            "Epoch 44/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9824Epoch 00044: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0845 - acc: 0.9823 - val_loss: 1.4584 - val_acc: 0.8467\n",
            "Epoch 45/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9799Epoch 00045: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 523us/step - loss: 0.0920 - acc: 0.9799 - val_loss: 1.5623 - val_acc: 0.8311\n",
            "Epoch 46/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9797Epoch 00046: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.0887 - acc: 0.9796 - val_loss: 1.4538 - val_acc: 0.8431\n",
            "Epoch 47/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9795Epoch 00047: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 519us/step - loss: 0.0895 - acc: 0.9793 - val_loss: 1.4925 - val_acc: 0.8359\n",
            "Epoch 48/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9819Epoch 00048: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0769 - acc: 0.9816 - val_loss: 1.3956 - val_acc: 0.8407\n",
            "Epoch 49/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9810Epoch 00049: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0870 - acc: 0.9810 - val_loss: 1.4526 - val_acc: 0.8419\n",
            "Epoch 50/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9803Epoch 00050: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 527us/step - loss: 0.0802 - acc: 0.9802 - val_loss: 1.4265 - val_acc: 0.8359\n",
            "Epoch 51/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9839Epoch 00051: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 517us/step - loss: 0.0685 - acc: 0.9837 - val_loss: 1.5012 - val_acc: 0.8455\n",
            "Epoch 52/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9821Epoch 00052: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 509us/step - loss: 0.0777 - acc: 0.9822 - val_loss: 1.5682 - val_acc: 0.8383\n",
            "Epoch 53/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9831Epoch 00053: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 520us/step - loss: 0.0764 - acc: 0.9829 - val_loss: 1.5024 - val_acc: 0.8395\n",
            "Epoch 54/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9816Epoch 00054: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.0845 - acc: 0.9817 - val_loss: 1.5760 - val_acc: 0.8395\n",
            "Epoch 55/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9856Epoch 00055: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.0601 - acc: 0.9853 - val_loss: 1.4796 - val_acc: 0.8443\n",
            "Epoch 56/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9846Epoch 00056: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 524us/step - loss: 0.0649 - acc: 0.9846 - val_loss: 1.4441 - val_acc: 0.8323\n",
            "Epoch 57/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9827Epoch 00057: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 523us/step - loss: 0.0698 - acc: 0.9829 - val_loss: 1.3831 - val_acc: 0.8455\n",
            "Epoch 58/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9849Epoch 00058: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.0612 - acc: 0.9847 - val_loss: 1.4764 - val_acc: 0.8419\n",
            "Epoch 59/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9839Epoch 00059: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0753 - acc: 0.9840 - val_loss: 1.4015 - val_acc: 0.8479\n",
            "Epoch 60/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9875Epoch 00060: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 527us/step - loss: 0.0509 - acc: 0.9874 - val_loss: 1.6373 - val_acc: 0.8371\n",
            "Epoch 61/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9842Epoch 00061: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 521us/step - loss: 0.0692 - acc: 0.9841 - val_loss: 1.5138 - val_acc: 0.8371\n",
            "Epoch 62/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9852Epoch 00062: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 523us/step - loss: 0.0672 - acc: 0.9853 - val_loss: 1.5847 - val_acc: 0.8359\n",
            "Epoch 63/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9853Epoch 00063: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 521us/step - loss: 0.0612 - acc: 0.9855 - val_loss: 1.5676 - val_acc: 0.8395\n",
            "Epoch 64/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9883Epoch 00064: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 518us/step - loss: 0.0588 - acc: 0.9880 - val_loss: 1.6064 - val_acc: 0.8311\n",
            "Epoch 65/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9861Epoch 00065: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0655 - acc: 0.9862 - val_loss: 1.6019 - val_acc: 0.8407\n",
            "Epoch 66/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9816Epoch 00066: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0742 - acc: 0.9817 - val_loss: 1.5920 - val_acc: 0.8419\n",
            "Epoch 67/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9863Epoch 00067: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 519us/step - loss: 0.0611 - acc: 0.9862 - val_loss: 1.5953 - val_acc: 0.8467\n",
            "Epoch 68/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9855Epoch 00068: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 524us/step - loss: 0.0671 - acc: 0.9856 - val_loss: 1.5302 - val_acc: 0.8407\n",
            "Epoch 69/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9883Epoch 00069: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 523us/step - loss: 0.0615 - acc: 0.9882 - val_loss: 1.5872 - val_acc: 0.8383\n",
            "Epoch 70/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9882Epoch 00070: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 524us/step - loss: 0.0555 - acc: 0.9880 - val_loss: 1.6563 - val_acc: 0.8371\n",
            "Epoch 71/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9861Epoch 00071: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.0700 - acc: 0.9861 - val_loss: 1.5618 - val_acc: 0.8431\n",
            "Epoch 72/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9853Epoch 00072: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0681 - acc: 0.9853 - val_loss: 1.5527 - val_acc: 0.8479\n",
            "Epoch 73/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9864Epoch 00073: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.0560 - acc: 0.9864 - val_loss: 1.6399 - val_acc: 0.8455\n",
            "Epoch 74/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9877Epoch 00074: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.0626 - acc: 0.9879 - val_loss: 1.7251 - val_acc: 0.8180\n",
            "Epoch 75/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9857Epoch 00075: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.0715 - acc: 0.9858 - val_loss: 1.7220 - val_acc: 0.8395\n",
            "Epoch 76/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9881Epoch 00076: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 517us/step - loss: 0.0549 - acc: 0.9882 - val_loss: 1.5710 - val_acc: 0.8455\n",
            "Epoch 77/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9847Epoch 00077: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.0662 - acc: 0.9847 - val_loss: 1.5572 - val_acc: 0.8467\n",
            "Epoch 78/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9878Epoch 00078: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0574 - acc: 0.9876 - val_loss: 1.7158 - val_acc: 0.8407\n",
            "Epoch 79/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9883Epoch 00079: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.0556 - acc: 0.9883 - val_loss: 1.6080 - val_acc: 0.8527\n",
            "Epoch 80/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9866Epoch 00080: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 524us/step - loss: 0.0634 - acc: 0.9867 - val_loss: 1.5149 - val_acc: 0.8455\n",
            "Epoch 81/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9890Epoch 00081: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 523us/step - loss: 0.0582 - acc: 0.9891 - val_loss: 1.5119 - val_acc: 0.8443\n",
            "Epoch 82/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9875Epoch 00082: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 523us/step - loss: 0.0598 - acc: 0.9877 - val_loss: 1.6273 - val_acc: 0.8467\n",
            "Epoch 83/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9858Epoch 00083: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.0698 - acc: 0.9859 - val_loss: 1.6313 - val_acc: 0.8455\n",
            "Epoch 84/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9884Epoch 00084: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 524us/step - loss: 0.0470 - acc: 0.9885 - val_loss: 1.6046 - val_acc: 0.8467\n",
            "Epoch 85/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9892Epoch 00085: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0536 - acc: 0.9892 - val_loss: 1.5993 - val_acc: 0.8491\n",
            "Epoch 86/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9893Epoch 00086: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 534us/step - loss: 0.0529 - acc: 0.9894 - val_loss: 1.5982 - val_acc: 0.8491\n",
            "Epoch 87/100\n",
            "6640/6680 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9887Epoch 00087: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 528us/step - loss: 0.0487 - acc: 0.9886 - val_loss: 1.5498 - val_acc: 0.8503\n",
            "Epoch 88/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9869Epoch 00088: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 524us/step - loss: 0.0653 - acc: 0.9867 - val_loss: 1.7609 - val_acc: 0.8383\n",
            "Epoch 89/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9884Epoch 00089: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.0595 - acc: 0.9883 - val_loss: 1.6834 - val_acc: 0.8419\n",
            "Epoch 90/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9878Epoch 00090: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 527us/step - loss: 0.0621 - acc: 0.9874 - val_loss: 1.6750 - val_acc: 0.8383\n",
            "Epoch 91/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9883Epoch 00091: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 527us/step - loss: 0.0618 - acc: 0.9879 - val_loss: 1.6178 - val_acc: 0.8383\n",
            "Epoch 92/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9882Epoch 00092: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 526us/step - loss: 0.0526 - acc: 0.9883 - val_loss: 1.5666 - val_acc: 0.8383\n",
            "Epoch 93/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9895Epoch 00093: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 527us/step - loss: 0.0545 - acc: 0.9897 - val_loss: 1.6699 - val_acc: 0.8491\n",
            "Epoch 94/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9890Epoch 00094: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 529us/step - loss: 0.0624 - acc: 0.9889 - val_loss: 1.6396 - val_acc: 0.8467\n",
            "Epoch 95/100\n",
            "6620/6680 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9875Epoch 00095: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 525us/step - loss: 0.0658 - acc: 0.9871 - val_loss: 1.5763 - val_acc: 0.8563\n",
            "Epoch 96/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9893Epoch 00096: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 530us/step - loss: 0.0544 - acc: 0.9891 - val_loss: 1.6233 - val_acc: 0.8527\n",
            "Epoch 97/100\n",
            "6660/6680 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9899Epoch 00097: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.0514 - acc: 0.9900 - val_loss: 1.7217 - val_acc: 0.8455\n",
            "Epoch 98/100\n",
            "6580/6680 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9894Epoch 00098: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 522us/step - loss: 0.0482 - acc: 0.9895 - val_loss: 1.6080 - val_acc: 0.8527\n",
            "Epoch 99/100\n",
            "6560/6680 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9893Epoch 00099: val_loss did not improve\n",
            "6680/6680 [==============================] - 3s 520us/step - loss: 0.0622 - acc: 0.9892 - val_loss: 1.6103 - val_acc: 0.8515\n",
            "Epoch 100/100\n",
            "6600/6680 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9876Epoch 00100: val_loss did not improve\n",
            "6680/6680 [==============================] - 4s 528us/step - loss: 0.0624 - acc: 0.9874 - val_loss: 1.6852 - val_acc: 0.8491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f45b91e14a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Srb34t8w_O_"
      },
      "source": [
        "### TODO: Load the model weights with the best validation loss.\n",
        "Xception_model.load_weights('saved_models/weights.best.Xception.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYcxkyRWw_PA",
        "outputId": "48377f3e-9783-4a45-cd61-7dfee62ebf1c"
      },
      "source": [
        "### TODO: Calculate classification accuracy on the test dataset.\n",
        "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\n",
        "test_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\n",
        "print('Test accuracy: %.4f%%' % test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 82.7751%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx2vHHew_PB"
      },
      "source": [
        "\n",
        "\n",
        "    extract_{network}\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gncgf3PZw_PB"
      },
      "source": [
        "### TODO: Write a function that takes a path to an image as input\n",
        "### and returns the dog breed that is predicted by the model.\n",
        "def Xception_predict_breed(img_path):\n",
        "    bottleneck_feature = extract_Xception(path_to_tensor(img_path))     # extract bottleneck features\n",
        "    predicted_vector = Xception_model.predict(bottleneck_feature)       # obtain predicted vector\n",
        "    return dog_names[np.argmax(predicted_vector)]                       # return dog breed that is predicted by the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwJjZJcgw_PC"
      },
      "source": [
        "### TODO: Write your algorithm.\n",
        "### Feel free to use as many code cells as needed.\n",
        "def display_img(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    imgplot = plt.imshow(cv_rgb)\n",
        "    return imgplot\n",
        "\n",
        "def predict_breed(img_path):\n",
        "    display_img(img_path)\n",
        "    if dog_detector(img_path):\n",
        "        print(\"Hey, It's a dog!\")\n",
        "        return print(\"And I guess the Breed of this dog is a {}\".format(Xception_predict_breed(img_path)))\n",
        "        \n",
        "    if face_detector(img_path):\n",
        "        print(\"Hey, It's a human!\")\n",
        "        return print(\"Mmmmmm....If you were a dog, I guess you would be a ... {}!!\".format(Xception_predict_breed(img_path)))\n",
        "        \n",
        "    else:\n",
        "        return print(\"Hum... seems this neither dog, nor human; must be something else .\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0_DJtxzw_PD",
        "outputId": "280586e9-ef42-454f-c8f9-c39d9d01fede"
      },
      "source": [
        "## TODO: Execute your algorithm from Step 6 on\n",
        "## at least 6 images on your computer.\n",
        "## Feel free to use as many code cells as needed.\n",
        "import numpy as np\n",
        "sample_files = np.array(glob(\"Dog-Breed-Classifier/test_images/*\"))\n",
        "print(sample_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q0S-i4Uw_PE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}